### 期末大作业
利用spark对纽约市出租车出行时间和空间数据进行分析

项目地址：https://gitee.com/ly10208/spark-taxi

#### 数据集选择

> ##### [TLC Trip Record Data](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page) 
>
> ##### 个人选取纽约市2020年1月到6月，绿色出租车的出行数据。该数据集大约104MHz左右，数据为.csv文件，总体应该有30多万行记录。
>
> ##### 每一条记录有21个字段，比较重要的有：上车时间（lpep_pickup_datetime）、下车时间（pep_dropoff_datetime）、上车地和下车地（PULocationID、DOLocationID）、出行距离（trip_distance）、各种费用（fare_amount）
>
> ##### 数据展示：

> ![image-20201128222317699](https://gitee.com/ly10208/images/raw/master/img/image-20201128222317699.png)
#### 分析目标

> ##### 大数据分析的基本思路不外乎三个关键词：分析历史、预测未来、优化选择。个人觉得这个出租车出行数据应该也不例外，我们已经拿到了2020年一月到六月的出行数据，所以我们可以对这些数据进行分析，找到一些规律，进而预测未来，或者优化一些选择。
>
> ##### 比如说：从时间上可以找出出行时间的密集时间段，出租车公司可以在这个时间段安排更多的司机在这个时间段上班，从而实现更大的客流载力；也可以找出出行时间最不密集的时间段，在这个时间段让一些出租车司机休息，从而降低运营成本；
>
> ##### 再比如说：从空间上找出上车人数最多的区域，可以安排更多的司机在该区域等候；找出上车人数最少的区域，吩咐司机尽量不要去该区域；
#### 实现过程

##### 读取hdfs数据

```scala
 val taxiRaw = sc.textFile("/user/stu2017210527/taxidata/trip_data_1.csv")
// 需要提前将数据集上传到hdfs中 /user/stu2017210527taxidata 目录下
```

##### 数据处理

```scala
// 预处理
//获取第一行数据
val firstLine = taxiRaw.first()    
//将第一行数据过滤掉 ，因为第一行数据是表头，所以需要先将其提取分离出来         
val taxiRdd = taxiRaw.filter(_ != firstLine) 

//定义sqlContex，便于后面将数据转成dataFrame，便于处理
val sqlContext = new org.apache.spark.sql.SQLContext(sc) 

// 转换成df
//将表头按","划分，成一个列表
val colName = firstLine.split(",")      
//定义数据结构，StructType表示每一行的数据类型，StructField表示一行中某个字段的类型（name、type、nullable）
val schema = StructType(colName.map(fieldName => StructField(fieldName, StringType, true)))
//分割数据，先将数据过滤，再分成一行一行的。t(8)表示distance
val rowRDD = taxiRdd.map(_.split(",")).filter(t => (t(8).toFloat > 0 && t(8).toFloat < 100)).map(p => Row(p: _*))
//利用sqlContext生成dataFrame格式的数据
val taxiDf = sqlContext.createDataFrame(rowRDD, schema)
```

##### Hbase配置

```scala
val conf = HBaseConfiguration.create()
//需要提前去Hbase创建  stu2017210527-data  表
// create 'stu2017210527-data0','v'
val tableName = "stu2017210527-data"     
val table = new HTable(conf,tableName)
conf.set(TableOutputFormat.OUTPUT_TABLE,tableName)

lazy val job = Job.getInstance(conf)
job.setMapOutputKeyClass(classOf[ImmutableBytesWritable])
job.setMapOutputValueClass(classOf[KeyValue])
HFileOutputFormat.configureIncrementalLoad(job,table)
```

##### 数据变换与存储

###### 地点

```scala
val loc = taxiDf.select("PULocationID")
//loc中每一行的数据数据格式为 [123]，变换过程中需要去掉 []
val taxiLoc = loc.map(line => {
    val word = line.toString()
    val key = word.substring(word.indexOf('[')+1,word.indexOf(']'))
    (key, 1)  //将去掉[]后的数据作为key，其value为1
 }).reduceByKey(_ + _) //根据key计数
                                            
//按照数量排序后存入hdfs文件
taxiLoc.sortBy(_._2).saveAsTextFile("/user/stu2017210527/spark/locationResult")

//将数据转换成可以写入hbase的格式                                           
val LocHbase = taxiLoc.sortBy(_._1).map{x=>{
//  一个 kv:KeyValue对象就是一行记录
//  所有插入的数据必须用org.apache.hadoop.hbase.util.Bytes.toBytes方法转换  
//  接收三个参数：列族，列名，数据
      val kv:KeyValue = new KeyValue(Bytes.toBytes("location"),Bytes.toBytes("v"),Bytes.toBytes(x._1),Bytes.toBytes(x._2));
    // 转化成RDD[(ImmutableBytesWritable,Put)]类型才能调用saveAsNewAPIHadoopFile写入  
      (new ImmutableBytesWritable(kv.getKey),kv)}}
                                            
//利用BulkLoad实现Hbase写入
//先直接生成一个基于hdfs的HFile文件，然后再将HFile数据文件移动到相应的Region上去，实现写入Hbase
LocHbase.saveAsNewAPIHadoopFile("hbase/loc",classOf[ImmutableBytesWritable],classOf[KeyValue],classOf[HFileOutputFormat],job.getConfiguration())
val bulkLoader = new LoadIncrementalHFiles(conf)
bulkLoader.doBulkLoad(new Path("hbase/loc"),table)
```

BulkLoad写入Hbase原理

HBase存储数据其底层使用的是HDFS来作为存储介质，HBase的每一张表对应的HDFS目录上的一个文件夹，文件夹名以HBase表进行命名（如果没有使用命名空间，则默认在default目录下），在表文件夹下存放在若干个Region命名的文件夹，Region文件夹中的每个列簇也是用文件夹进行存储的，每个列簇中存储就是实际的数据，以HFile的形式存在

![image-20201128213619369](https://gitee.com/ly10208/images/raw/master/img/image-20201128213619369.png)

###### 时间

和上面地点差不多，只是这里既分析了**小时级数据和十分钟级数据**

```scala
val time = taxiDf.select("lpep_pickup_datetime")
val taxiTimeHours = time.map(line => {
    val key = line.toString().substring(12, 14) //hours
    //      val key = line.toString().substring(12,16) //ten mins
    //      val key = line.toString().substring(12,17) // mins
    (key, 1)
}).reduceByKey(_ + _).sortBy(_._1)

taxiTimeHours.saveAsTextFile("/user/stu2017210527/spark/timeResult")

val timeHoursHbase = taxiTimeHours.map{x=>{
    val kv:KeyValue = new KeyValue(Bytes.toBytes("hours"),Bytes.toBytes("v"),Bytes.toBytes(x._1),Bytes.toBytes(x._2+""));
    (new ImmutableBytesWritable(kv.getKey),kv)}}

timeHoursHbase.saveAsNewAPIHadoopFile("hbase/hours",classOf[ImmutableBytesWritable],classOf[KeyValue],classOf[HFileOutputFormat],job.getConfiguration())
bulkLoader.doBulkLoad(new Path("hbase/hours"),table)


val taxiTimeMins = time.map(line => {
    val key = line.toString().substring(12,16) //ten mins
    (key, 1)
}).reduceByKey(_ + _).sortBy(_._1)
taxiTimeMins.saveAsTextFile("/user/stu2017210527/spark/timeMinsResult")

val timeMinsHbase = taxiTimeMins.map{x=>{
    val kv:KeyValue = new KeyValue(Bytes.toBytes("hours:mins"),Bytes.toBytes("v"),Bytes.toBytes(x._1),Bytes.toBytes(x._2+""));
    (new ImmutableBytesWritable(kv.getKey),kv)}}

timeMinsHbase.saveAsNewAPIHadoopFile("hbase/mins",classOf[ImmutableBytesWritable],classOf[KeyValue],classOf[HFileOutputFormat],job.getConfiguration())
bulkLoader.doBulkLoad(new Path("hbase/mins"),table)
```

#### 最终结果

##### HDFS

###### 地点

![image-20201128214423009](https://gitee.com/ly10208/images/raw/master/img/image-20201128214423009.png)

###### 时间

![image-20201128214545924](https://gitee.com/ly10208/images/raw/master/img/image-20201128214545924.png)

##### 可视化

###### 小时级

![image-20201128214719253](https://gitee.com/ly10208/images/raw/master/img/image-20201128214719253.png)

###### 十分钟级

![image-20201128214750700](https://gitee.com/ly10208/images/raw/master/img/image-20201128214750700.png)

##### Hbase

###### 时间

![image-20201129102055848](https://gitee.com/ly10208/images/raw/master/img/20201129102055.png)

![image-20201129102139834](https://gitee.com/ly10208/images/raw/master/img/20201129102139.png)

###### 地点

![image-20201129102233731](https://gitee.com/ly10208/images/raw/master/img/20201129102233.png)

#### 遇到问题

1. ##### 

![image-20201128215709395](https://gitee.com/ly10208/images/raw/master/img/image-20201128215709395.png)

解决：map中做过多的数据类型转换，实在要做可以先将数据转换成df格式

​           直接写入Hbase时，一次性写入数据量可能不宜过大

2. 

![image-20201128220205466](https://gitee.com/ly10208/images/raw/master/img/image-20201128220205466.png)

解决：要存入Hbase的rdd必须以key进行排序，而不能用value排序